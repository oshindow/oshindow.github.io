<!DOCTYPE html>
<html lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171609755-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171609755-1');
</script>
<title>Xintong Wang 王心童</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="w3.css">
<link rel="stylesheet" href="style.css">
<link rel='stylesheet' href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap">
<!-- <link rel="icon" href="media/image/favicon.png"> -->
</head>

<body class="w3-light-grey">

<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:1400px;">

  <!-- The Grid -->
    <div class="w3-row-padding">

        <!-- The Main Body -->
        <div class="w3-container">
            <h2>Xintong Wang, <span lang="zh-Hans">王心童</span></h2>
        </div>

        <div class="w3-container">
            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>About</h3>
                <p>I am a first-year PhD student at NUS, supervised by Prof. Wang Ye. I earned my B.S. from Beijing Forestry University in 2022. My research interests broadly focus on the speech and singing voice related perception and synthesis, such as speech recognition, speech synthesis, singing voice synthesis, and voice conversion</p>
                <p>My <a href="media/cv/cv_240502.pdf" target="_blank"> cv</a> is here. If you would like to reach out to me, my email address is <u><i>xintongwang9709 at gmail dot com</i></u>. </p>    
            </div>

            <!-- <div class="w3-container w3-card w3-white w3-margin-bottom">
                    <h3>Education</h3>
                    
                    <p>Ph.D. in Computer Science, <a>National University of Singapore</a>, 2024 - 2028</p>
                    <p>B.S. in Mathematics and Applied Mathematics, <a>Beijing Forestry University</a>, 2018 - 2022</p>
            </div> -->

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Work Experience</h3>
                <p>National University of Singapore</p>
                <ol start="1">
                    <p>Research Assistant @ School of Computing (SMC Lab), Singapore, Oct 2023 – Aug 2024</p>
                </ol>
                <p>Xiaoice</p>
               Also known as 小冰 in Chinese and りんな in Japanese
                <ol start="1">
                    <p>Speech Synthsis Engineer @ Xiaoice (X·studio), Beijing, Jul 2022 – Oct 2023</p>
                    <p>Speech Recognition Intern @ Xiaoice (AI Being BU), Beijing, May 2021 – Jul 2022</p>
                </ol>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Publications</h3>
                <!-- <p>Under Review</p>
                <ol start="1"></ol> -->
                <p>Journal Articles</p>
                <ol start="1">
                    <li><b>Xintong Wang</b>, Chuangang Zhao, "A 2D Convolutional Gating Mechanism for Mandarin Streaming Speech Recognition,"
                        <i>Information</i>, 12.4 (2021): 165. 
                        <a class="block" href="media/publication/information-12-00165-v2.pdf" target="_blank">PDF</a>
                    
                        <!-- <a class="block" href="https://shaojinding.github.io/samples/accentron/" target="_blank">demo</a></li> -->
                    <!-- <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "Converting foreign accent speech without a reference," <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>, vol. 29, pp. 2367–2381, 2021.
                        <a class="block" href="media/publication/zhao2021taslp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://guanlongzhao.github.io/demo/reference-free-ac/" target="_blank">demo</a></li>
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "The English pronunciation of Arabic speakers: A data-driven approach to segmental error identification," <i>Language Teaching Research</i>, 
                        2020. <a class="block" href="media/publication/lucic2020arabic.pdf" target="_blank">pdf</a> <a class="block" href="media/publication/lucic2020arabic_summary.pdf" target="_blank">summary</a></li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "Using phonetic posteriorgram based frame pairing for segmental accent conversion," <i>IEEE/ACM Transactions on Audio, 
                        Speech, and Language Processing</i>, vol. 27, no. 10, pp. 1649–1660, 2019. <a class="block" href="media/publication/zhao2019taslp.pdf" target="_blank">pdf</a> <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> <a class="block" 
                        href="https://guanlongzhao.github.io/demo/ppg-gmm/" target="_blank">demo</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "Learning structured sparse representations for voice conversion," <i>IEEE/ACM Transactions 
                        on Audio, Speech, and Language Processing</i>, vol. 28, pp. 343–354, 2019. <a class="block" href="media/publication/ding2019taslp.pdf" target="_blank">pdf</a> <a class="block" href="https://shaojinding.github.io/samples/cssr/cssr_demo" target="_blank">demo</a></li>
                    <li>S. Ding, C. Liberatore, S. Sonsaat, I. Lučić Rehman, A. Silpachai, <b>G. Zhao</b>, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "Golden speaker 
                        builder–An interactive tool for pronunciation training," <i>Speech Communication</i>, vol. 115, pp. 51–66, 2019. <a class="block" href="media/publication/ding2019gsb.pdf" target="_blank">pdf</a> 
                        <a class="block" href="https://github.com/shaojinding/Golden-Speaker-Builder" target="_blank">code</a>
                        <a class="block" href="https://goldenspeaker.engl.iastate.edu/speech/" target="_blank">demo</a></li> -->
                </ol>
                <p>Workshop Articles</p>
                <ol start="1">
                <li><b>Xintong Wang</b>, Chang Zeng, Jun Chen, and Chunhui Wang, "Crosssinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice Synthesizer Trained on Monolingual Singers."
                    <i> 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</i>
                    <a class="block" href="https://wavelandspeech.github.io/CrossSinger/" target="_blank">Demo</a>  
                    <a class="block" href="https://arxiv.org/abs/2309.12672/" target="_blank">PDF</a></li>  
                </ol>
                <p>Conference Articles</p>
                <ol start="1">
                <li><b>Xintong Wang</b>, Mingqian Shi, and Ye Wang, "Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and Diagnosis,"
                    <i>Interspeech 2024</i> 
                    <a class="block" href="media/publication/Interspeech_2024__Mandarin_Chinese_MDD_camera_ready3.pdf" target="_blank">PDF</a></li> 
                
                </ol>
                <!-- <p>Conference Proceedings</p>
                <ol start="7">
                    <li><b>G. Zhao</b>, Q. Wang, H. Lu, Y. Huang, and I. Lopez Moreno, "Augmenting transformer-transducer based speaker change detection with token-level training loss,"
                        in <i>IEEE International Conference on Acoustics, Speech, and 
                            Signal Processing (ICASSP)</i>, 2023. <a class="block" href="media/publication/zhao2022augmenting.pdf" target="_blank">pdf</a>
                            <a class="block" href="media/publication/zhao2023icassp_poster.pdf" target="_blank">poster</a>
                        <a class="block" href="https://github.com/google/speaker-id/tree/master/publications/ScdLoss" target="_blank">resources</a></li>
                    <li>B. Labrador<sup>*</sup>, <b>G. Zhao</b><sup>*</sup>, I. Lopez Moreno<sup>*</sup>, A. Scorza Scarpati, L. Fowl, and Quan Wang, "Exploring sequence-to-sequence transformer-transducer models for keyword spotting,"
                        in <i>IEEE International Conference on Acoustics, Speech, and 
                            Signal Processing (ICASSP)</i>, 2023. <sup>*</sup>Equal contribution. <a class="block" href="media/publication/labrador2022exploring.pdf" target="_blank">pdf</a></li>
                    <li>A. Hair, <b>G. Zhao</b>, B. Ahmed, K. Ballard, and R. Gutierrez-Osuna, "Assessing posterior-based mispronunciation detection on field-collected recordings from child speech therapy sessions,"
                        in <i>Interspeech</i>, 2021. pp. 2936–2940. <a class="block" href="media/publication/hair2021interspeech.pdf" target="_blank">pdf</a></li>
                    <li>A. Silpachai, I. Lučić Rehman, T. A. Barriuso, J. Levis, E. Chukharev-Khudilaynen, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Effects of voice type and task on L2 learners' awareness of pronunciation errors,"
                        in <i>Interspeech</i>, 2021. pp. 1952–1956. <a class="block" href="media/publication/silpachai2021interspeech.pdf" target="_blank">pdf</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Improving the speaker identity of non-parallel many-to-many voice conversion with adversarial speaker recognition," in <i>Interspeech</i>, 2020. pp. 776–780.
                        <a class="block" href="media/publication/ding2020interspeech.pdf" target="_blank">pdf</a> <a class="block" href="https://github.com/shaojinding/Adversarial-Many-to-Many-VC" target="_blank">code</a> 
                        <a class="block" href="https://shaojinding.github.io/samples/adv/" target="_blank">demo</a> 
                        <a class="block" href="https://interspeech2020.oss-cn-beijing.aliyuncs.com/Mon/Mon-2-7-2.mp4" target="_blank">video</a></li>
                    <li>A. Das, <b>G. Zhao</b>, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, "Understanding the effect of voice quality and accent on talker similarity," in <i>Interspeech</i>, 2020. pp. 1763–1767.
                        <a class="block" href="media/publication/das2020interspeech.pdf" target="_blank">pdf</a> 
                        <a class="block" href="https://interspeech2020.oss-cn-beijing.aliyuncs.com/Tue/Tue-1-7-9.mp4" target="_blank">video</a></li>
                    <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "Foreign accent conversion by synthesizing speech from phonetic posteriorgrams," 
                        in <i>Interspeech</i>, 2019, pp. 2843–2847. <a class="block" href="media/publication/zhao2019interspeech.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/fac-via-ppg" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/fac-via-ppg/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2019interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li><b>G. Zhao</b>, S. Sonsaat, A. Silpachai, I. Lučić Rehman, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "L2-ARCTIC: A non-native English 
                        speech corpus," in <i>Interspeech</i>, 2018, pp. 2783–2787. <a class="block" href="media/publication/zhao2018interspeech.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank">data</a> <a class="block" href="https://github.com/guanlongzhao/kaldi-gop" target="_blank">code</a> 
                        <a class="block" href="media/publication/zhao2018interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "Improving sparse representations in exemplar-based voice conversion with a 
                        phoneme-selective objective function," in <i>Interspeech</i>, 2018, pp. 476–480. <a class="block" href="media/publication/ding2018interspeech.pdf" target="_blank">pdf</a></li>
                    <li>C. Liberatore, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Voice conversion through residual warping in a sparse, anchor-based representation of speech," 
                        in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 2018, pp. 5284–5288. <a class="block" href="media/publication/liberatore2018icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="media/publication/liberatore2018icassp_poster.pdf" target="_blank">poster</a></li>
                    <li><b>G. Zhao</b>, S. Sonsaat, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, "Accent conversion using phonetic posteriorgrams," 
                        in <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2018, pp. 5314–5318. <a class="block" href="media/publication/zhao2018icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/icassp18/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2018icassp_poster.pdf" target="_blank">poster</a></li>
                    <li>G. Angello, A. B. Manam, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Training behavior of successful tacton-phoneme learners," 
                        in <i>IEEE Haptics Symposium (WIP)</i>, 2018. <a class="block" href="media/publication/angello2018haptics.pdf" target="_blank">pdf</a></li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "Exemplar selection methods in voice conversion," in <i>IEEE International Conference on Acoustics, Speech, and 
                        Signal Processing (ICASSP)</i>, 2017, pp. 5525–5529. <a class="block" href="media/publication/zhao2017icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://guanlongzhao.github.io/demo/icassp17/" target="_blank">demo</a> 
                        <a class="block" href="media/publication/zhao2017icassp_poster.pdf" target="_blank">poster</a></li>
                </ol>
                <p>Book Chapter</p>
                <ol start="20">
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "Chapter 10 — Image dehazing: Improved techniques,"
                        in <i>Deep Learning through Sparse and Low-Rank Modeling</i>: Elsevier, 2019, pp. 251–262.
                        <a class="block" href="https://www.sciencedirect.com/science/article/pii/B9780128136591000196" target="_blank">link</a>
                        <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                </ol>
                <p>Preprints</p>
                <ol start="21">
                    <li>Q. Wang, Y. Huang, H. Lu, <b>G. Zhao</b>, and I. Lopez Moreno, "Highly efficient real-time streaming and fully on-device speaker diarization with multi-stage clustering,"
                        <i>arXiv preprint arXiv:2210.13690</i>, 2022. <a class="block" href="media/publication/wang2022highly.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/wq2012/SpectralCluster" target="_blank">code</a></li>
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "Improved techniques for learning to dehaze and 
                        beyond: A collective study," <i>arXiv preprint arXiv:1807.00202</i>, 2018. <a class="block" href="media/publication/liu2018dehaze2.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                    <li>Y. Liu and <b>G. Zhao</b>, "PAD-Net: A perception-aided single image dehazing network," <i>arXiv preprint arXiv:1805.03146</i>, 2018. 
                        <a class="block" href="media/publication/liu2018dehaze1.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/single-image-dehazing" target="_blank">code</a></li>
                    <li>A. Datta, <b>G. Zhao</b>, B. Ramabhadran, E. Weinstein, "LSTM acoustic models learn to align and pronounce with graphemes," <i>arXiv preprint arXiv:2008.06121</i>, 2020.
                        (Work done as an intern at Google NYC during summer 2018.)
                        <a class="block" href="media/publication/datta2020arxiv.pdf" target="_blank">pdf</a></li>
                </ol>
                <p>Abstracts</p>
                <ol start="25">
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Pronunciation errors — A systematic approach to diagnosis," 
                    in <i>L2 Pronunciation Research Workshop: Bridging the Gap between Research and Practice</i>, 2019, pp. 23–24. <a class="block" href="media/publication/lucic2019l2prw.pdf" target="_blank">pdf</a></li>
                    <li>S. Sonsaat, E. Chukharev-Hudilainen, I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, S. Ding, C. Liberatore, and R. Gutierrez-Osuna, "Golden Speaker Builder, an interactive tool for pronunciation training: User 
                        studies," in <i>6th International Conference on English Pronunciation: Issues & Practices (EPIP6)</i>, 2019, p. 72. <a class="block" href="media/publication/sonsaat2019epip6.pdf" target="_blank">pdf</a></li>
                    <li>S. Ding, C. Liberatore, <b>G. Zhao</b>, S. Sonsaat, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "Golden Speaker Builder: an interactive online tool for L2 learners to build pronunciation models,"
                        in <i>Pronunciation in Second Language Learning and Teaching (PSLLT)</i>, 2017, pp. 25–26. <a class="block" href="media/publication/ding2017gsb.pdf" target="_blank">pdf</a></li>
                </ol> -->
            </div>

            <!-- <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Professional Service</h3>
                <p>Reviewer for:</p>
                <ul>
                    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">IEEE Transactions on Image Processing</a></li>
                    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206" target="_blank">IEEE Transactions on Information Forensics and Security</a></li>
                    <li><a href="https://www.cell.com/heliyon/home" target="_blank">Heliyon</a></li>
                    <li><a href="https://www.lltjournal.org/" target="_blank">Language Learning & Technology</a></li>
                    <li><a href="https://www.sciencedirect.com/journal/computer-speech-and-language" target="_blank">Computer Speech & Language</a></li>
                    <li><a href="https://www.aimspress.com/journal/MBE" target="_blank">Mathematical Biosciences and Engineering</a></li>
                    <li><a href="https://www.aimspress.com/journal/era" target="_blank">Electronic Research Archive</a></li>
                    <li>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP): <a href="https://2020.ieeeicassp.org/" target="_blank">2020</a>, <a href="https://2022.ieeeicassp.org/" target="_blank">2022</a>, <a href="https://2023.ieeeicassp.org/" target="_blank">2023</a> (<a href="media/others/outstanding_reviewer_icassp23.pdf" target="_blank">Outstanding Reviewer</a>)</li>
                    <li>International Conference on Advances in Signal, Image and Video Processing (SIGNAL): <a href="https://www.iaria.org/conferences2020/SIGNAL20.html" target="_blank">2020</a>, <a href="https://www.iaria.org/conferences2021/SIGNAL21.html" target="_blank">2021</a>, <a href="https://www.iaria.org/conferences2022/ComSIGNAL22.html" target="_blank">2022</a>, <a href="https://www.iaria.org/conferences2023/ComSIGNAL23.html" target="_blank">2023</a></li>
                    <li>Annual Conference of the International Speech Communication Association (Interspeech): <a href="https://interspeech2019.org/" target="_blank">2019</a>, <a href="https://interspeech2021.org/" target="_blank">2021</a>,
                        <a href="https://interspeech2022.org/" target="_blank">2022</a>, <a href="https://interspeech2023.org/" target="_blank">2023</a></li>
                    <li>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU): <a href="https://asru2021.signalprocessingsociety.org/asru2021.org/index.html" target="_blank">2021</a>, <a href="http://www.asru2023.org/" target="_blank">2023</a></li>
                    <li>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA): <a href="https://signalprocessingsociety.org/blog/waspaa-2023-2023-ieee-workshop-applications-signal-processing-audio-and-acoustics" target="_blank">2023</a></li>
                </ul>
                <p>Students mentored:</p>
                <ul>
                    <li><a href="https://ana-kuznetsova.github.io/" target="_blank">Anastasia Kuznetsova</a>, Research Intern @ Google, 2023</li>
                    <li><a href="https://scholar.google.com/citations?hl=en&user=biA-c9sAAAAJ" target="_blank">Beltrán Labrador</a>, Research Intern @ Google, 2022, 2023</li>
                </ul>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Teaching</h3>
                <p>Teaching Assistant: <a href="https://people.engr.tamu.edu/rgutier/web_courses/csce482_s16/contact.htm" target="_blank">CSCE 482: Senior Capstone Design (Spring 2016)</a></p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Honors</h3>
                <ul>
                    <li>Graduate Student Travel Award (for Interspeech'19), Department of Computer Science and Engineering, Texas A&M University, 2019</li>
                    <li>Graduate Student Presentation Grant (for ICASSP'17), Office of Graduate and Professional Studies, Texas A&M University, 2017</li>
                    <li>Outstanding Graduate Award, University of Science and Technology of China, 2015</li>
                    <li>Outstanding Undergraduate Student Scholarship, University of Science and Technology of China, 2011–2014</li>
                    <li>Second Prize @ Chinese Chemistry Olympiad (Provincial Level), Chinese Chemical Society, 2010</li>
                </ul>
            </div> -->

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <!-- <h3>Visitors</h3> -->
                <a href="https://info.flagcounter.com/cjrd"><img src="https://s11.flagcounter.com/count2/cjrd/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="1"></a>
            </div>

        <!-- End Main Body -->
        </div>

    <!-- End Grid -->
    </div>
  
  <!-- End Page Container -->
</div>

<footer class="w3-container w3-center">
    <p><a href="https://oshindow.github.io/" target="_blank">Personal Website</a> | 
        <!-- <a href="https://www.linkedin.com/in/guanlongzhao/" target="_blank">LinkedIn</a> |  -->
        <a href="https://github.com/oshindow" target="_blank">GitHub</a> | 
        <a href="notebook/git.html" target="_blank">Notebook</a> | 
        <a href="demo/fac-via-ppg/index.html" target="_blank">Demos</a> | 
        <a href="https://scholar.google.com/citations?hl=en&user=oQH2fkAAAAAJ&view_op=list_works&gmla=AOV7GLPstHuSkn5xUP3asnKDd6GAmAM9NYsjarAX2DwSOF8d94ZKmfCJN4kMNI9FnB5UUmV5Ki6hCRZ-Fn9If3qYPzRHLQZAQNEi6p4bCLuj4NgphVrGlVt8M8lKx0OLHOXzxa-7SSGCCJCwFnbuPYHbstagRA" target="_blank">Google Scholar</a> | 
        <!-- <a href="https://psi.engr.tamu.edu/people/guanlong-zhao/" target="_blank">PSI Lab</a></p> -->
    <p>Copyright &copy <script>document.write(new Date().getFullYear())</script> Xintong Wang. All Rights Reserved.</p>
</footer>

</body>
</html>
